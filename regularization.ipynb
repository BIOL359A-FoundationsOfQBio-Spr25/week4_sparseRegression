{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A | Regularization\n",
    "### Spring 2025, Week 4\n",
    "Objectives:\n",
    "- Understand the Bias-Variance Tradeoff: Learn how increasing model complexity impacts bias and variance.\n",
    "- Gain intuition for Regularization Techniques: Implement Ridge and LASSO to prevent overfitting.\n",
    "- Implement Model Evaluation and Selection: Employ train-test splits for optimal model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will start by working with in-silico data. The code below will generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, n_features=10, noise_level=0.5, random_state=42, add_n_extreme=0):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with controlled noise and multicollinearity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    n_features : int\n",
    "        Number of features to generate\n",
    "    noise_level : float\n",
    "        Standard deviation of the Gaussian noise\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    add_extreme : bool\n",
    "        If True, adds an extreme outlier value to the last sample\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Feature matrix\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target vector\n",
    "    beta : ndarray of shape (n_features,)\n",
    "        True coefficients\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate random feature matrix\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Add multicollinearity: Make some features correlated\n",
    "    X[:, 5] = 0.7 * X[:, 0] + 0.3 * np.random.randn(n_samples)\n",
    "    X[:, 6] = 0.8 * X[:, 1] + 0.2 * np.random.randn(n_samples)\n",
    "    X[:, 7] = 0.9 * X[:, 2] + 0.1 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Generate true coefficients with sparsity (some are zero)\n",
    "    beta = np.zeros(n_features)\n",
    "    beta[0] = 1.5\n",
    "    beta[1] = -2.0\n",
    "    beta[2] = 3.0\n",
    "    beta[3] = -0.5\n",
    "    beta[4] = 1.0\n",
    "    # Features 5, 6, 7 have zero coefficients (but are correlated with others)\n",
    "    beta[8] = -1.0\n",
    "    beta[9] = 2.0\n",
    "    \n",
    "    # Generate target variable with noise\n",
    "    y = np.dot(X, beta) + noise_level * np.random.randn(n_samples)\n",
    "    # Add an extreme outlier if requested\n",
    "    if add_n_extreme:\n",
    "        y[-add_n_extreme:] += noise_level * (np.random.rand(add_n_extreme)) * 50\n",
    "        #X[-add_n_extreme:] += np.random.randn(add_n_extreme, n_features) * 10\n",
    "    \n",
    "    return X, y, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "n_samples = 100\n",
    "n_features = 10\n",
    "noise_level = 0.5\n",
    "X, y, true_beta = generate_data(n_samples, n_features, noise_level, add_n_extreme=10)\n",
    "true_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a linear regression model that will help us learn about the system that generated these data. To make the coefficients of our model more comparable to each other, we need to start by standardizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features for better convergence in regularization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Before standarization:\")\n",
    "print(f\"mean: {np.mean(X_train): .3f}, std: {np.std(X_train):.3f}\")\n",
    "print(\"After standarization:\")\n",
    "print(f\"mean: {np.mean(X_train_scaled): .3f}, std: {np.std(X_train_scaled):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mlr(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Fit Multiple Linear Regression model and return metrics\"\"\"\n",
    "    mlr = LinearRegression()\n",
    "    mlr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = mlr.predict(X_train)\n",
    "    y_test_pred = mlr.predict(X_test)\n",
    "    \n",
    "    # Performance metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    return mlr.coef_, train_mse, test_mse, train_r2, test_r2\n",
    "\n",
    "def plot_mlr_results(noise_level=0.5, n_samples=100, add_n_extreme=0):\n",
    "    \"\"\"Plot MLR results with the given noise level and sample size\"\"\"\n",
    "    # Generate new data with the specified parameters\n",
    "    X, y, true_beta = generate_data(n_samples=n_samples, noise_level=noise_level, add_n_extreme=add_n_extreme)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit MLR\n",
    "    coef, train_mse, test_mse, train_r2, test_r2 = fit_mlr(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Coefficient values comparison (True vs Estimated)\n",
    "    feature_indices = np.arange(len(true_beta))\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    ax1.bar(feature_indices - bar_width/2, true_beta, bar_width, label='True Coefficients', color='blue', alpha=0.7)\n",
    "    ax1.bar(feature_indices + bar_width/2, coef, bar_width, label='Estimated Coefficients', color='red', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Feature Index')\n",
    "    ax1.set_ylabel('Coefficient Value')\n",
    "    ax1.set_title('True vs. Estimated Coefficients in MLR')\n",
    "    ax1.set_xticks(feature_indices)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Predicted vs Actual values\n",
    "    ax2.scatter(y_train, np.dot(X_train_scaled, coef), color='k', alpha=0.7, label=\"Train\")\n",
    "    ax2.scatter(y_test, np.dot(X_test_scaled, coef), color='green', alpha=0.7, label=\"Test\")\n",
    "    \n",
    "    # Add a diagonal line (perfect predictions)\n",
    "    min_val_train = min(min(y_train), min(np.dot(X_train_scaled, coef)))\n",
    "    min_val_test = min(min(y_test), min(np.dot(X_test_scaled, coef)))\n",
    "    min_val = min(min_val_train, min_val_test)\n",
    "    max_val_train = max(max(y_train), max(np.dot(X_train_scaled, coef)))\n",
    "    max_val_test = max(max(y_test), max(np.dot(X_test_scaled, coef)))\n",
    "    max_val = max(max_val_test, max_val_test)\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "    \n",
    "    ax2.set_xlabel('Actual Values')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title('Predicted vs. Actual Values in MLR')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    # Add text with performance metrics\n",
    "    textstr = '\\n'.join((\n",
    "        f'Training MSE: {train_mse:.2f}',\n",
    "        f'Test MSE: {test_mse:.2f}',\n",
    "        f'Training R²: {train_r2:.2f}',\n",
    "        f'Test R²: {test_r2:.2f}'))\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax2.text(0.05, 0.95, textstr, transform=ax2.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive MLR visualization\n",
    "print(\"Multiple Linear Regression Interactive Visualization\")\n",
    "print(\"===================================================\")\n",
    "print(\"Adjust the noise level and sample size to see how they affect MLR performance.\")\n",
    "\n",
    "interact(plot_mlr_results,\n",
    "         add=widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=0.5, description='Noise Level:'),\n",
    "         noise_level=widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=0.5, description='Noise Level:'),\n",
    "         n_samples=widgets.IntSlider(min=50, max=500, step=50, value=100, description='Sample Size:'),\n",
    "         add_n_extreme=widgets.IntSlider(min=0, max=10, step=1, value=0, description='N Outliers:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "The interactive visualization below performs Ridge regression on these data. Use the sliders to adjust the polynomial degree and the lambda value, which controls the strength of the regularization:\n",
    "\n",
    "- Polynomial Degree: Influences model complexity. Lower degrees might not capture all the data variability (underfitting), while higher degrees might model the noise as well (overfitting).\n",
    "- Lambda Value: Adjusts the Ridge regularization strength. Increasing lambda enhances the model's generalization by penalizing large coefficients, helping to prevent overfitting.\n",
    "\n",
    "Observe how adjusting the lambda values impacts the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X_train, y_train, X_test, y_test, alpha):\n",
    "    \"\"\"Fit Ridge Regression model and return metrics\"\"\"\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = ridge.predict(X_train)\n",
    "    y_test_pred = ridge.predict(X_test)\n",
    "    \n",
    "    # Performance metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    return ridge.coef_, train_mse, test_mse, train_r2, test_r2\n",
    "\n",
    "def plot_ridge_results(alpha=1.0, noise_level=0.5, n_samples=100, add_n_extreme=0):\n",
    "    \"\"\"Plot Ridge Regression results with the given parameters\"\"\"\n",
    "    # Generate new data with the specified parameters\n",
    "    X, y, true_beta = generate_data(n_samples=n_samples, noise_level=noise_level, add_n_extreme=add_n_extreme)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit MLR for comparison\n",
    "    mlr_coef, mlr_train_mse, mlr_test_mse, mlr_train_r2, mlr_test_r2 = fit_mlr(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    mlr_y_test_pred = np.dot(X_test_scaled, mlr_coef)\n",
    "    # Fit Ridge\n",
    "    ridge_coef, ridge_train_mse, ridge_test_mse, ridge_train_r2, ridge_test_r2 = fit_ridge(X_train_scaled, y_train, X_test_scaled, y_test, alpha)\n",
    "    ridge_y_test_pred = np.dot(X_test_scaled, ridge_coef)\n",
    "\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Coefficient values comparison\n",
    "    feature_indices = np.arange(len(true_beta))\n",
    "    bar_width = 0.25\n",
    "    \n",
    "    ax1.bar(feature_indices - bar_width, true_beta, bar_width, label='True Coefficients', color='blue', alpha=0.7)\n",
    "    ax1.bar(feature_indices, mlr_coef, bar_width, label='MLR Coefficients', color='red', alpha=0.7)\n",
    "    ax1.bar(feature_indices + bar_width, ridge_coef, bar_width, label='Ridge Coefficients', color='green', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Feature Index')\n",
    "    ax1.set_ylabel('Coefficient Value')\n",
    "    ax1.set_title(f'Coefficient Comparison (Ridge with α={alpha:.2f})')\n",
    "    ax1.set_xticks(feature_indices)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Regression results - Predicted vs Actual\n",
    "    ax2.scatter(y_test, mlr_y_test_pred, color='red', alpha=0.7, label='MLR Predictions')\n",
    "    ax2.scatter(y_test, ridge_y_test_pred, color='green', alpha=0.7, label='Ridge Predictions')\n",
    "    \n",
    "    # Add a diagonal line (perfect predictions)\n",
    "    min_val = min(min(y_test), min(min(mlr_y_test_pred), min(ridge_y_test_pred)))\n",
    "    max_val = max(max(y_test), max(max(mlr_y_test_pred), max(ridge_y_test_pred)))\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Predictions')\n",
    "    \n",
    "    ax2.set_xlabel('Actual Values')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title('Predicted vs. Actual Values: MLR vs Ridge')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add text with performance metrics\n",
    "    mlr_metrics = f'MLR: MSE={mlr_test_mse:.2f}, R²={mlr_test_r2:.2f}'\n",
    "    ridge_metrics = f'Ridge: MSE={ridge_test_mse:.2f}, R²={ridge_test_r2:.2f}'\n",
    "    \n",
    "    ax2.text(0.05, 0.95, mlr_metrics, transform=ax2.transAxes, fontsize=10,\n",
    "            verticalalignment='top', color='red', \n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax2.text(0.05, 0.87, ridge_metrics, transform=ax2.transAxes, fontsize=10,\n",
    "            verticalalignment='top', color='green',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display regularization effect summary\n",
    "    print(f\"Ridge Regression (α={alpha:.2f}) Summary:\")\n",
    "    print(f\"Sum of square of MLR parameters: {np.sum(mlr_coef**2):.4f}\")\n",
    "    print(f\"Sum of square of Ridge parameters: {np.sum(ridge_coef**2):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Interactive Ridge Regression visualization\n",
    "print(\"Ridge Regression Interactive Visualization\")\n",
    "print(\"==========================================\")\n",
    "print(\"Adjust the alpha parameter to control regularization strength,\")\n",
    "print(\"and other parameters to see their effect on Ridge Regression performance.\")\n",
    "\n",
    "interact(plot_ridge_results, \n",
    "         alpha=widgets.FloatLogSlider(min=-2, max=3, value=1.0, base=10, description='Alpha (λ):'),\n",
    "         noise_level=widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=0.5, description='Noise Level:'),\n",
    "         n_samples=widgets.IntSlider(min=50, max=500, step=50, value=100, description='Sample Size:'),\n",
    "         add_n_extreme=widgets.IntSlider(min=0, max=10, step=1, value=0, description='N Outliers:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso\n",
    "\n",
    "This interactive visualization showcases Lasso regression applied to polynomial models. Adjust the polynomial degree and alpha value using the provided sliders to explore their effects:\n",
    "\n",
    "- Polynomial Degree: Controls the complexity of the model. Lower degrees might result in underfitting, where the model is too simplistic, while higher degrees might lead to overfitting, capturing noise instead of just the underlying data pattern.\n",
    "- Lambda Value: Manages the Lasso regularization strength, promoting sparsity in the model coefficients. Higher lambda values can lead to more coefficients being reduced to zero, simplifying the model and potentially improving its generalizability.\n",
    "\n",
    "Observe how changing the lambda value affects the number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lasso(X_train, y_train, X_test, y_test, alpha):\n",
    "    \"\"\"Fit Lasso Regression model and return metrics\"\"\"\n",
    "    # Note: For numerical stability, we might need to increase max_iter\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000, tol=0.0001)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = lasso.predict(X_train)\n",
    "    y_test_pred = lasso.predict(X_test)\n",
    "    \n",
    "    # Performance metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    return lasso.coef_, train_mse, test_mse, train_r2, test_r2\n",
    "\n",
    "def plot_lasso_results(alpha=0.1, noise_level=0.5, n_samples=100, add_n_extreme=0):\n",
    "    \"\"\"Plot Lasso Regression results with the given parameters\"\"\"\n",
    "    # Generate new data with the specified parameters\n",
    "    X, y, true_beta = generate_data(n_samples=n_samples, noise_level=noise_level, add_n_extreme=add_n_extreme)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Fit MLR for comparison\n",
    "    mlr_coef, mlr_train_mse, mlr_test_mse, mlr_train_r2, mlr_test_r2 = fit_mlr(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    # Fit Lasso\n",
    "    lasso_coef, lasso_train_mse, lasso_test_mse, lasso_train_r2, lasso_test_r2 = fit_lasso(X_train_scaled, y_train, X_test_scaled, y_test, alpha)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Coefficient values comparison\n",
    "    feature_indices = np.arange(len(true_beta))\n",
    "    bar_width = 0.25\n",
    "    \n",
    "    ax1.bar(feature_indices - bar_width, true_beta, bar_width, label='True Coefficients', color='blue', alpha=0.7)\n",
    "    ax1.bar(feature_indices, mlr_coef, bar_width, label='MLR Coefficients', color='red', alpha=0.7)\n",
    "    ax1.bar(feature_indices + bar_width, lasso_coef, bar_width, label='Lasso Coefficients', color='purple', alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Feature Index')\n",
    "    ax1.set_ylabel('Coefficient Value')\n",
    "    ax1.set_title(f'Coefficient Comparison (Lasso with α={alpha:.4f})')\n",
    "    ax1.set_xticks(feature_indices)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Feature selection visualization\n",
    "    # Show zero vs non-zero coefficients\n",
    "    zero_mask = np.abs(lasso_coef) < 1e-10\n",
    "    nonzero_mask = ~zero_mask\n",
    "    \n",
    "    selected_features = np.sum(nonzero_mask)\n",
    "    zero_coef_count = np.sum(zero_mask)\n",
    "    \n",
    "    ax2.pie([selected_features, zero_coef_count], \n",
    "            labels=[f'Non-zero Coefficients ({selected_features})', \n",
    "                   f'Zero Coefficients ({zero_coef_count})'],\n",
    "            colors=['purple', 'gray'], \n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            explode=(0.1, 0))\n",
    "    \n",
    "    ax2.set_title('Lasso Feature Selection')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display model comparison\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'MLR': [mlr_train_mse, mlr_test_mse, mlr_train_r2, mlr_test_r2],\n",
    "        'Lasso': [lasso_train_mse, lasso_test_mse, lasso_train_r2, lasso_test_r2]\n",
    "    }, index=['Train MSE', 'Test MSE', 'Train R²', 'Test R²'])\n",
    "    \n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Display sparsity analysis\n",
    "    print(f\"Lasso Regression (α={alpha:.4f}) Sparsity Analysis:\")\n",
    "    print(f\"Number of non-zero coefficients: {selected_features} out of {len(lasso_coef)}\")\n",
    "    print(f\"L1 Penalty Term: {np.sum(np.abs(lasso_coef)):.4f}\")\n",
    "    print(f\"Non-zero coefficients indices: {np.where(nonzero_mask)[0].tolist()}\")\n",
    "    print(f\"True non-zero coefficients indices: {np.where(np.abs(true_beta) > 0)[0].tolist()}\")\n",
    "\n",
    "# Interactive Lasso Regression visualization\n",
    "print(\"\\nLasso Regression Interactive Visualization\")\n",
    "print(\"==========================================\")\n",
    "print(\"Adjust the alpha parameter to control sparsity,\")\n",
    "print(\"and other parameters to see their effect on Lasso Regression performance.\")\n",
    "\n",
    "interact(plot_lasso_results, \n",
    "         alpha=widgets.FloatLogSlider(min=-3, max=1, value=0.1, base=10, description='Alpha (λ):'),\n",
    "         noise_level=widgets.FloatSlider(min=0.1, max=2.0, step=0.1, value=0.5, description='Noise Level:'),\n",
    "         n_samples=widgets.IntSlider(min=50, max=500, step=50, value=100, description='Sample Size:'),\n",
    "         add_n_extreme=widgets.IntSlider(min=0, max=10, step=1, value=0, description='N Outliers:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "- Bias: Bias is the error introduced by approximating a real-world problem, which might be highly complex, with a simpler model. It reflects how well the model captures the underlying patterns of the data. High bias can cause the model to miss relevant relations between features and target outputs (underfitting).\n",
    "\n",
    "- Variance: Variance measures how much the model's predictions vary for a given dataset. A model with high variance pays a lot of attention to training data and learns noise as well as signals, leading to less generalizable models on new data (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples=100, degree=3, noise_level=0.5, x_range=(-3, 3)):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with polynomial relationship and controlled noise.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate\n",
    "    degree : int\n",
    "        True polynomial degree of the data\n",
    "    noise_level : float\n",
    "        Standard deviation of the Gaussian noise\n",
    "    x_range : tuple\n",
    "        Range of x values (min, max)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray of shape (n_samples,)\n",
    "        Feature values\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target values with noise\n",
    "    true_coef : ndarray\n",
    "        True coefficients used to generate data\n",
    "    \"\"\"\n",
    "    # Generate random x values within the specified range\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(x_range[0], x_range[1], n_samples)\n",
    "    \n",
    "    # Generate random coefficients for polynomial\n",
    "    true_coef = np.random.randn(degree + 1)\n",
    "    true_coef = true_coef / np.max(np.abs(true_coef)) * 3  # Scale coefficients\n",
    "    \n",
    "    # Generate y values based on polynomial relationship\n",
    "    y_true = np.zeros(n_samples)\n",
    "    for i in range(degree + 1):\n",
    "        y_true += true_coef[i] * X**i\n",
    "    \n",
    "    # Add noise\n",
    "    y = y_true + noise_level * np.random.randn(n_samples)\n",
    "    \n",
    "    return X, y, true_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit polynomial regression model with various degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit polynomial regression models\n",
    "def fit_polynomial_regression(X, y, max_degree=10, if_lasso=False, if_ridge=False):\n",
    "    \"\"\"\n",
    "    Fit polynomial regression models of varying degrees.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray of shape (n_samples,)\n",
    "        Feature values\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Target values\n",
    "    max_degree : int\n",
    "        Maximum polynomial degree to fit\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    models : list\n",
    "        List of fitted models for each degree\n",
    "    \"\"\"\n",
    "    # Reshape X for sklearn\n",
    "    X_reshaped = X.reshape(-1, 1)\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    # Fit models for degrees 1 to max_degree\n",
    "    for degree in range(1, max_degree + 1):\n",
    "        # Create polynomial features\n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "        X_poly = poly_features.fit_transform(X_reshaped)\n",
    "        \n",
    "        # Fit linear regression model\n",
    "        if if_lasso:\n",
    "          model = Lasso(alpha=1.0, max_iter=10000, tol=0.0001)\n",
    "        elif if_ridge:\n",
    "          model = Ridge(alpha=1.0, max_iter=10000, tol=0.0001)\n",
    "        else:\n",
    "          model = LinearRegression()\n",
    "        model.fit(X_poly, y)\n",
    "        \n",
    "        # Store model and transformer\n",
    "        models.append((model, poly_features))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Filter out the specific ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "# Function to visualize polynomial regression results\n",
    "def visualize_polynomial_regression(X_train, y_train, X_test, y_test, models, true_degree, true_coef):\n",
    "    \"\"\"\n",
    "    Visualize polynomial regression models of different degrees.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : ndarray\n",
    "        Training data\n",
    "    X_test, y_test : ndarray\n",
    "        Test data\n",
    "    models : list\n",
    "        List of fitted models for each degree\n",
    "    true_degree : int\n",
    "        True polynomial degree of the data\n",
    "    true_coef : ndarray\n",
    "        True coefficients used to generate data\n",
    "    \"\"\"\n",
    "    # Create a smooth curve for plotting\n",
    "    X_curve = np.linspace(min(X_train.min(), X_test.min()), max(X_train.max(), X_test.max()), 1000).reshape(-1, 1)\n",
    "    \n",
    "    # Calculate true function values\n",
    "    y_true_curve = np.zeros(len(X_curve))\n",
    "    for i in range(true_degree + 1):\n",
    "        y_true_curve += true_coef[i] * X_curve.flatten()**i\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 1: Data and model fits\n",
    "    ax = axes[0]\n",
    "    ax.scatter(X_train, y_train, color='blue', alpha=0.5, label='Training Data')\n",
    "    ax.scatter(X_test, y_test, color='green', alpha=0.5, label='Test Data')\n",
    "    ax.plot(X_curve, y_true_curve, 'k--', linewidth=2, label='True Function')\n",
    "    \n",
    "    # Select specific degrees to plot (to avoid cluttering)\n",
    "    degrees_to_plot = [1, true_degree, 10] if true_degree != 1 and true_degree != 10 else [1, 5, 10]\n",
    "    colors = ['red', 'purple', 'orange']\n",
    "    \n",
    "    for i, degree in enumerate(degrees_to_plot):\n",
    "        if degree <= len(models):\n",
    "            model, poly_features = models[degree - 1]\n",
    "            X_curve_poly = poly_features.transform(X_curve)\n",
    "            y_pred = model.predict(X_curve_poly)\n",
    "            ax.plot(X_curve, y_pred, color=colors[i], linewidth=2, label=f'Degree {degree}')\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Polynomial Fits of Different Degrees')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training and test errors vs. polynomial degree\n",
    "    ax = axes[1]\n",
    "    degrees = list(range(1, len(models) + 1))\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    \n",
    "    for degree in range(1, len(models) + 1):\n",
    "        model, poly_features = models[degree - 1]\n",
    "        \n",
    "        # Reshape data for transformation\n",
    "        X_train_reshaped = X_train.reshape(-1, 1)\n",
    "        X_test_reshaped = X_test.reshape(-1, 1)\n",
    "        \n",
    "        # Transform data\n",
    "        X_train_poly = poly_features.transform(X_train_reshaped)\n",
    "        X_test_poly = poly_features.transform(X_test_reshaped)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred = model.predict(X_train_poly)\n",
    "        y_test_pred = model.predict(X_test_poly)\n",
    "        \n",
    "        # Calculate errors\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        \n",
    "        train_errors.append(train_mse)\n",
    "        test_errors.append(test_mse)\n",
    "    \n",
    "    ax.plot(degrees, train_errors, 'b-', marker='o', label='Training Error')\n",
    "    ax.plot(degrees, test_errors, 'r-', marker='s', label='Test Error')\n",
    "    ax.axvline(x=true_degree, color='k', linestyle='-.', alpha=0.7, label=f'True Degree = {true_degree}')\n",
    "    \n",
    "    # Find the degree with minimum test error\n",
    "    best_degree = degrees[np.argmin(test_errors)]\n",
    "    ax.axvline(x=best_degree, color='g', linestyle='--', alpha=0.7, label=f'Best Degree = {best_degree}')\n",
    "    \n",
    "    ax.set_xlabel('Polynomial Degree')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.set_title('Error vs. Model Complexity')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Bias and variance components (simplified representation)\n",
    "    ax = axes[2]\n",
    "    \n",
    "    biases = np.array(train_errors)\n",
    "    \n",
    "    # Simplified variance calculation (increasing with complexity)\n",
    "    variances = np.array([max(0.01, (test_err - train_err)) for train_err, test_err in zip(train_errors, test_errors)])\n",
    "    \n",
    "    # Total error = bias^2 + variance\n",
    "    total_errors = biases + variances\n",
    "    \n",
    "    ax.plot(degrees, biases, 'b-', marker='o', label='Bias')\n",
    "    ax.plot(degrees, variances, 'r-', marker='s', label='Variance')\n",
    "    ax.plot(degrees, total_errors, 'g-', marker='^', label='Total Error')\n",
    "    ax.axvline(x=true_degree, color='k', linestyle='--', alpha=0.7, label=f'True Degree = {true_degree}')\n",
    "    \n",
    "    ax.set_xlabel('Polynomial Degree')\n",
    "    ax.set_ylabel('Error Component')\n",
    "    ax.set_title('Bias-Variance Decomposition')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive visualization function\n",
    "def interactive_polynomial_regression():\n",
    "    @interact(true_degree=widgets.IntSlider(min=1, max=9, step=1, value=3, description='True Degree:'),\n",
    "              noise_level=widgets.FloatSlider(min=0.1, max=20.0, step=0.1, value=0.5, description='Noise Level:'),\n",
    "              n_samples=widgets.IntSlider(min=20, max=200, step=10, value=100, description='Sample Size:'),\n",
    "              regularization=widgets.RadioButtons(\n",
    "                  options=['None', 'Lasso', 'Ridge'],\n",
    "                  value='None',\n",
    "                  description='Regularization:',\n",
    "                  disabled=False\n",
    "              ))\n",
    "    def update(true_degree=3, noise_level=0.5, n_samples=100, regularization='None'):\n",
    "        # Generate data\n",
    "        X, y, true_coef = generate_data(n_samples=n_samples, degree=true_degree, noise_level=noise_level)\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Determine regularization type\n",
    "        if_lasso = (regularization == 'Lasso')\n",
    "        if_ridge = (regularization == 'Ridge')\n",
    "        \n",
    "        # Fit models of different degrees\n",
    "        max_degree = 10\n",
    "        models = fit_polynomial_regression(X_train, y_train, max_degree=max_degree, if_lasso=if_lasso, if_ridge=if_ridge)\n",
    "        \n",
    "        # Visualize results\n",
    "        visualize_polynomial_regression(X_train, y_train, X_test, y_test, models, true_degree, true_coef)\n",
    "\n",
    "print(\"True Degree: The actual polynomial degree of the data-generating process\")\n",
    "print(\"Noise Level: The amount of random noise added to the data\")\n",
    "print(\"Sample Size: The number of data points generated\")\n",
    "\n",
    "interactive_polynomial_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
